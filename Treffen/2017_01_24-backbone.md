# Backbone Treffen

## IP6to4
* Es gibt Probleme mit IP6to4, nachdem die Gateways jetzt IPv6 Verbindungen akzeptieren
* Vorschlag: 6to4 IPs sperren (null routen)
* Umsetzung: IPTables Eintrag für fastd Port (6to4 Adressen rejecten)

## VPN Problem
* Alle VPNs ausser 1,3 und 6 sind problematisch
* VPN 04 GRE Tunnel nach Rheinland ist OK
    * VPN04 Tunnel ist falsch konfiguriert
    * JPLitza schreibt Mail
* VPN 02 bald kein Exit mehr
* VPN 05 Hide.me Tunnel ist Gülle
* VPN 01 + 03 Hostsystem ist überlastet (load 5)
* Grafana Statistik hat nun load für Gateways ([Titel: Load vs. Neighbours](https://grafana.bremen.freifunk.net/dashboard/db/vpn-server))
* Messungen (IPerf) von JPlitza ergeben, dass Performanceprobleme beim VPN (Exit?)/fastd bestehen
* Neben-Vorschlag: auf Gateways Fastd-Prozesse auf mehreren Ports starten, um alle CPUs auszulasten
* MTR von vpn01 ins Internet zeigt, dass schon zum ersten Hop (Router-VM auf dem gleichen Host) 20ms Latenz sind (außerdem Packet Loss)
    * MTR auf auf den anderen Gateways zeigt, dass es auf vpn01, 03 und 04 zu hohe Latenz gibt, und auf den anderen nicht
    * vpn01, 03 und 04 haben vermutlich zu hohe CPU-Last
* Vermutung: Host-System (wo vpn01, vpn03, etc. laufen) ist überlastet
* Vermutung: Routing der vielen Pakete braucht zuviel CPU
    * evtl. zieht die ffmap-VM (z.B. Generierung von oldstyle-Graph-Bildern) zu viel CPU (in Spikes)
* Maßnahme: Generierung von RRDs und oldstyle-RRD-Graph-Bildern wurde abgeschaltet
    * MTR zeigt leichte Verbesserungen, muss aber immer noch besser werden
    * wget auf vpn01 zeigt Verbesserungen
* Maßnahme: ffmap-Ressourcen gedrosselt (weniger Kerne)
* -> Ergebnis: NIC vom Host-System direkt in die Vyatta-VM weiterreichen, um Anzahl der Bridges zu reduzieren
* es gehen wohl generell zuviele Pakete über die VPN-Server

* Fazit: Es gibt zu viele Management-Pakete ca. Faktor 10 (zehnmal soviele Management Pakete wie unicast Nutzdaten)
* die VPNs sind zwar tw. langsam und müllig, aber sind wohl im Moment nicht das Hauptproblem

* Multicast-Verkehr scheint tödlich zu sein:
    * laut Grafana haben wir abends ca. 250 Multicast-Pakete/Sekunde (das ist auch realistisch)
    * bei 67 Clients an VPN01 -> 16750 Pakete, die pro Sekunde rausgehen müssen
    * das deckt sich mit den Messwerten auf VPN01
* Maßnahmen gegen Multicast-Problem:
    * Filtern auf VPN-Servern:
        * von jplitza abgelehnt
        * technisch derzeit nicht möglich, weil wir dazu den Inhalt der Batman-Pakete untersuchen müssten
        * (genofire Nachwort: Die Multicast Pakete werden so über mehr meshe geleitet, vermuttung wäre eine höhere Last innerhalb eines Meshes)
    * Filtern auf Knoten:
        * erfordert FW-Update
        * Ergebnisse werden erst sichtbar, wenn eine große Menge Knoten aktualisiert werden; es müssen also auch Stable-Knoten aktualisiert werden
        * (genofire Nachwort: so gestalten, das sammtlicher multicast im Mesh funktioniert und beim Routen im VPN-Tunnel verworfen wird -> Technisch umsetzbar?)
    * Segmentierung des ganzen FFHB-Netzes
        * technisch aufwändig (erfordert einiges an Planung)
    * mehr Hardware (mehr/stärkere Gateways), um den MC-Verkehr zu verkraften
        * dann wird das Problem evtl. nur auf die Knoten verlagert (die dann ~250 Packets/Second verarbeiten müssen)
* selbst wenn wir Multicast gezielt filtern, bleibt noch der IPv6 Neighbour Solicitation Traffic (den wir nicht wegfiltern wollen)


* Beobachtung: die VPN-Server haben sehr unterschiedlich viele Clients
    * wird vermutl. durch Provider Steering verursacht

* weiterer Server:
    * war ja eh angedacht :-)
    * wir könnten dann die "performance-kritischen" Dienste (BGP-Router etc.) von anderen Diensten (Webserver, ffmap) trennen
    * dadurch könnte das Routing beschleunigt werden
    * derzeit wird die Host-CPU wohl praktisch komplett unter vpn01, vpn03 und Vyatta aufgeteilt; durch Verlagerung von Gateways auf andere physikalische Hosts könnte das Routing beschleunigt werden
* -> Ergebnis: es wird geschaut, ob wir noch einen weitere physikalischen Host aufsetzen können, wo das BGP-Routing (derzeit Vyatta) dann nativ läuft (außerdem dann von Vyatta auf normales Linux wechseln)
    * Ziel: mit hoher Paketmenge besser umgehen können, weniger Packet Loss und weniger Latenz

* -> Ergebnis: wir müssen uns eine bessere Lösung für Multicast-Traffic überlegen, bevor das Netz komplett unbenutzbar wird


## Performance-Messungen
* VPN01: 
    * 7.2MBit/s jplitza Zuhause über Internet nach vpn01 (nur Upload, iperf)
    * 6.35MBit/s jplitza Zuhause über Internet nach Hostsystem (nur Upload, iperf)
    * 4 MB/s jplitza Zuhause über Internet nach vpn01 (nur Download, curl)
    * 1 MB/s jplitza Zuhause über Fastd+Batman nach vpn01 (nur Download, curl)
    * 1 MB/s jplitza Zuhause über Fastd (ohne Batman) nach vpn01 (nur Download, curl)
    * 2 MB/s jplitza Zuhause über Fastd+Batman nach vpn01 (nur Download, curl) nachdem RRD abgeschaltet wurde
* VPN02:
* VPN03:
* VPN04:
* VPN05: 6.5MB/s zu 1G Speedtestdatei durch hide.me Tunnel
* VPN06:

## Worauf wollen wir das Netz optimieren?
Worauf soll das Provider Steering optimiert werden? Latenz, Bandbreite, Packet Loss?

* Vorschlag: mit PS-Einstellungen rumspielen (z.B. bestimmte Server mal bevorzugen) und beobachten, wie sich Durchsatz etc. verändert

## Grafana Statistik der VPNs
* Traffic in Grafana ist mit ohne Overhead und so

Schreibpad: https://hackmd.io/AwDgnA7ATArKC0BGGBjAzPALAUzSeARgYgGzxQAmEFFwiKBEKAhkA===?edit#